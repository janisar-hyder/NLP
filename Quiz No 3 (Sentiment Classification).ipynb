{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>`Assignment 3`</font>\n",
    "##    <font color=red>Submitted By:</font> M.Janisar\n",
    "##    <font color=red>Registration No:</font> SP22-BCS-047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BASIC IMPORTS and File Reading`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . ', 'pos'), ('the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth . ', 'pos'), ('effective but too-tepid biopic', 'pos'), ('if you sometimes like to go to the movies to have fun , wasabi is a good place to start . ', 'pos'), (\"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \", 'pos')]\n",
      "10662\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pickle\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell_Check = Speller()\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = 'D:\\\\NLP\\\\' \n",
    "filepath = os.listdir(path)\n",
    "\n",
    "short_pos = open(\"D:\\\\NLP\\\\pos.txt\", \"r\").read()\n",
    "short_neg = open(\"D:\\\\NLP\\\\neg.txt\", \"r\").read()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append((r, \"pos\"))\n",
    "\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append((r, \"neg\"))\n",
    "\n",
    "print(documents[:5])\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Preprocessing and Frequency Distribution`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52052\n",
      "50136\n",
      "Length of BOW: 14691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_words = []\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc if not (token.is_stop or not token.is_alpha)]\n",
    "    lemmas=[token.lemma_ for token in nlp(' '.join(tokens))]\n",
    "    return lemmas\n",
    "\n",
    "short_pos_words = preprocessing(short_pos)\n",
    "short_neg_words = preprocessing(short_neg)\n",
    "print(len(short_pos_words))\n",
    "print(len(short_neg_words))\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print(\"Length of BOW: \" + str(len(all_words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Pickling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfile=open(\"Featuressss.pickle\",\"wb\")\n",
    "pickle.dump(all_words,newfile)\n",
    "newfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile=open(\"Featuressss.pickle\",\"rb\")\n",
    "all_words=pickle.load(myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Printing Keys After Preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ['rock', 'destine', 'century', 'new', 'conan', 'going', 'splash', 'great', 'arnold', 'schwarzenegger', 'jean', 'claud', 'van', 'damme', 'steven', 'segal', 'gorgeously', 'elaborate', 'continuation', 'lord', 'ring', 'trilogy', 'huge', 'column', 'word', 'adequately', 'describe', 'co', 'writer', 'director', 'peter', 'jackson', 'expand', 'vision', 'j', 'r', 'tolkien', 'middle', 'earth', 'effective', 'tepid', 'biopic', 'like', 'movie', 'fun', 'wasabi', 'good', 'place', 'start', 'emerge', 'rare', 'issue', 'honest', 'keenly', 'observed', 'feel', 'film', 'provide', 'insight', 'neurotic', 'mindset', 'comic_strip', 'reach', 'absolute', 'game', 'offer', 'combination', 'entertainment', 'education', 'picture', 'literally', 'show', 'road', 'hell', 'pave', 'intention', 'steer', 'turn', 'snappy', 'screenplay', 'curl', 'edge', 'clever', 'want', 'hate', 'pull', 'care', 'cat', 'refreshingly', 'different', 'slice', 'asian', 'cinema', 'worth', 'see', 'talk', 'singing', 'head', 'surprise', 'wisegirl', 'low', 'key', 'quality', 'genuine', 'tenderness', 'wendigo', 'fed', 'eye', 'heart', 'mind', 'family', 'orient', 'fantasy', 'adventure', 'ultimately', 'ponder', 'reason', 'need', 'story', 'utterly', 'compelling', 'write', 'reputation', 'famous', 'author', 'live', 'come', 'question', 'illuminate', 'overly', 'talky', 'documentary', 'masterpiece', 'year', 'make', 'ripe', 'enrapture', 'beauty', 'tempt', 'willing', 'probe', 'inscrutable', 'mystery', 'breath', 'fresh', 'air', 'true', 'sophistication', 'thoughtful', 'provocative', 'insistently', 'humanizing', 'cast', 'include', 'actor', 'work', 'independent', 'lovely', 'amazing', 'involve', 'incisive', 'bleakly', 'amusing', 'life', 'disturb', 'frighteningly', 'evocative', 'assembly', 'imagery', 'hypnotic', 'music', 'compose', 'philip', 'glass', 'connect', 'nice', 'departure', 'standard', 'moviegoe', 'fare', 'score', 'point', 'dedicated', 'hearted', 'professionalism', 'occasionally', 'melodramatic', 'extremely', 'spiderman', 'idealistic', 'love', 'bring', 'latent', 'old', 'romantic', 'minute', 'treasure', 'planet', 'maintain', 'brisk', 'pace', 'race', 'familiar', 'lack', 'grandeur', 'epic', 'associate', 'stevenson', 'tale', 'early', 'disney', 'effort', 'help', 'lil', 'bow', 'wow', 'tone', 'pint', 'sized', 'gangsta', 'act', 'play', 'resemble', 'real', 'kid', 'guarantee', 'shake', 'rattle', 'roll', 'masterful', 'master', 'filmmaker', 'unique', 'deceptive', 'grimness', 'fatalist', 'worldview', 'light', 'cute', 'forgettable', 'way', 'effectively', 'teach', 'kids', 'danger', 'drug', 'think', 'project', 'unfortunately', 'rate', 'pay', 'easy', 'crush', 'title', 'wedding', 'funeral', 'far', 'hugh', 'grant', 'whimsy', 'literate', 'smart', 'take', 'static', 'cantet', 'perfectly', 'capture', 'hotel', 'lobby', 'lane', 'highway', 'roadside', 'cafe', 'permeate', 'vincent', 'day', 'ms', 'fulford', 'wierzbicki', 'spooky', 'sulky', 'calculate', 'lolita', 'mean', 'laissez', 'passer', 'distinguished', 'distinctive', 'bona', 'fide', 'fascinating', 'replete', 'reward', 'reap', 'bond', 'outing', 'recent', 'stunt', 'outlandish', 'border', 'cartoonlike', 'heavy', 'reliance', 'cgi', 'technology', 'begin', 'creep', 'series', 'newton', 'draw', 'attention', 'magnet', 'circle', 'well', 'know', 'star', 'mark', 'wahlberg', 'lose', 'bite', 'happy', 'end', 'plausible', 'rest', 'novel', 'ride', 'fuller', 'surely', 'call', 'gutsy', 'time', 'exhilarate', 'yarn', 'compleja', 'e', 'intelectualmente', 'retadora', 'el', 'de', 'es', 'uno', 'esos', 'filme', 'que', 'vale', 'la', 'pena', 'ver', 'precisamente', 'por', 'su', 'originalidad', 'strong', 'case', 'importance', 'musician', 'create', 'motown', 'sound', 'karman', 'move', 'rhythm', 'lip', 'chant', 'beat', 'long', 'braid', 'hair', 'little', 'wipe', 'away', 'jewel', 'bead', 'sweat', 'gosling', 'performance', 'dwarf', 'people', 'give', 'glimpse', 'culture', 'tender', 'lacerate', 'darkly', 'funny', 'fable', 'spoof', 'target', 'giant', 'creature', 'feature', 'acknowledge', 'celebrate', 'cheesiness', 'kick', 'watch', 'today', 'engage', 'overview', 'johnson', 'eccentric', 'career', 'rag', 'cheap', 'unassuming', 'charisma', 'listen', 'read', 'phone', 'book', 'sandra', 'bullock', 'likeable', 'nettelbeck', 'beautifully', 'orchestrate', 'transformation', 'chilly', 'self', 'absorb', 'martha', 'open', 'snow', 'lovable', 'siberian', 'husky', 'plus', 'sheep', 'dog', 'host', 'parka', 'wrap', 'dose', 'everytime', 'undercover', 'brother', 'run', 'steam', 'find', 'amuse', 'manage', 'original', 'rip', 'idea', 'singer', 'composer', 'bryan', 'adam', 'contribute', 'slew', 'song', 'potential', 'hit', 'simply', 'intrusive', 'package', 'certainly', 'intend', 'er', 'spirit', 'piece', 'america', 'plucky', 'british', 'eccentrics', 'hearts', 'gold', 'charming', 'enlightened', 'derrida', 'lecture', 'undeniably', 'fascinate', 'playful', 'fellow', 'pleasant', 'hold', 'skilled', 'ensemble', 'american', 'trouble', 'teen', 'miss', 'beloved', 'screen', 'tuck', 'everlaste', 'labour', 'layered', 'richness', 'chiaroscuro', 'madness', 'astonish', 'animated', 'subplot', 'depict', 'inner', 'struggle', 'adolescent', 'hero']\n"
     ]
    }
   ],
   "source": [
    "lof = 500\n",
    "word_features = list(all_words.keys())[:lof]\n",
    "print(len(word_features) , word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Finding Features (True/False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature sets: 10662\n"
     ]
    }
   ],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "print(\"Length of feature sets: \" + str(len(featuresets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 56.48551410717667\n",
      "MNB_classifier accuracy percent: 53.010793410338955\n",
      "BernoulliNB_classifier accuracy percent: 56.40977087672789\n",
      "LogisticRegression_classifier accuracy percent: 56.864230259420566\n",
      "SGDClassifier_classifier accuracy percent: 57.46070819920469\n",
      "SVC_classifier accuracy percent: 56.59912895284984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Janisar Hyder\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 57.19560689263397\n",
      "NuSVC_classifier accuracy percent: 57.18613898882787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ratio1 = 80\n",
    "ratio2 = 20\n",
    "\n",
    "training_set = featuresets[:int((lof * ratio1) / 100)]\n",
    "testing_set = featuresets[int((lof * ratio2) / 100):]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>`Assignment # 4`</font>\n",
    "##    <font color=red>Submitted By:</font> M.Janisar\n",
    "##    <font color=red>Registration No:</font> SP22-BCS-047\n",
    "##    <font color=white>______________________________________</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Preprocessing & Feature Finding`     <font color=red></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20003\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import Speller\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "    \n",
    "        \n",
    "short_pos = open(\"D:\\\\NLP\\\\pos.txt\",\"r\").read()\n",
    "short_neg = open(\"D:\\\\NLP\\\\neg.txt\",\"r\").read()\n",
    "short_nuet = open(\"D:\\\\NLP\\\\nue.txt\",\"r\").read()\n",
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "def preprocessing(text):\n",
    "    tokens = [token.text.lower() for token in nlp(text) if not token.is_stop and token.is_alpha]\n",
    "    lemmas = [token.lemma_ for token in nlp(' '.join(tokens))]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "for p in short_pos.split('\\n'):\n",
    "    documents.append((p, \"pos\"))\n",
    "    lemmas = preprocessing(p)\n",
    "    for w in lemmas:\n",
    "        all_words.append(w)\n",
    "\n",
    "for p in short_neg.split('\\n'):\n",
    "    documents.append((p, \"neg\"))\n",
    "    lemmas = preprocessing(p)\n",
    "    for w in lemmas:\n",
    "         all_words.append(w)\n",
    "\n",
    "for p in short_nuet.split('\\n'):\n",
    "    documents.append((p, \"nue\"))\n",
    "    lemmas = preprocessing(p)\n",
    "    for w in lemmas:\n",
    "         all_words.append(w)\n",
    "\n",
    "\n",
    "save_documents = open(\"documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()\n",
    "\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "\n",
    "save_word_features = open(\"word_features5k.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()\n",
    "\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "featuresets_save = open(\"featuressss.pickle\",\"wb\")\n",
    "pickle.dump(featuresets, featuresets_save)\n",
    "featuresets_save.close()\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))\n",
    "\n",
    "testing_set = featuresets[1000:]\n",
    "training_set = featuresets[:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Preprocessing & Feature Finding`     <font color=red>[accuracy, precision, recall, f1]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ibrahim Qureshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ibrahim Qureshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier           Accuracy   Precision  Recall     F1 Score   Average   \n",
      "Naive Bayes          73.30      67.19      73.30      69.74      70.89\n",
      "Multinomial NB       80.00      74.04      80.00      71.14      76.29\n",
      "Bernoulli NB         80.01      64.02      80.01      71.13      73.79\n",
      "Logistic Regression  78.40      65.82      78.40      70.77      73.35\n",
      "Linear SVC           70.50      65.94      70.50      68.04      68.75\n",
      "SGD                  68.81      66.14      68.81      67.38      67.78\n",
      "\n",
      "Best classifier based on average score: Multinomial NB\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def evaluate_classifier(clf, test_set):\n",
    "    predictions = [clf.classify(sample[0]) for sample in test_set]\n",
    "    gold = [sample[1] for sample in test_set]\n",
    "    accuracy = nltk.classify.accuracy(clf, test_set)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(gold, predictions, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "results = {}\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "accuracy, precision, recall, f1 = evaluate_classifier(classifier, testing_set)\n",
    "results['Naive Bayes'] = [accuracy, precision, recall, f1]\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "accuracy, precision, recall, f1 = evaluate_classifier(MNB_classifier, testing_set)\n",
    "results['Multinomial NB'] = [accuracy, precision, recall, f1]\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "accuracy, precision, recall, f1 = evaluate_classifier(BernoulliNB_classifier, testing_set)\n",
    "results['Bernoulli NB'] = [accuracy, precision, recall, f1]\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "accuracy, precision, recall, f1 = evaluate_classifier(LogisticRegression_classifier, testing_set)\n",
    "results['Logistic Regression'] = [accuracy, precision, recall, f1]\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "accuracy, precision, recall, f1 = evaluate_classifier(LinearSVC_classifier, testing_set)\n",
    "results['Linear SVC'] = [accuracy, precision, recall, f1]\n",
    "\n",
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "accuracy, precision, recall, f1 = evaluate_classifier(SGDC_classifier, testing_set)\n",
    "results['SGD'] = [accuracy, precision, recall, f1]\n",
    "\n",
    "average_scores = {clf: sum(metrics)/len(metrics) for clf, metrics in results.items()}\n",
    "best_classifier = max(average_scores, key=average_scores.get)\n",
    "\n",
    "print(\"{:<20} {:<10} {:<10} {:<10} {:<10} {:<10}\".format('Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Average'))\n",
    "for clf, metrics in results.items():\n",
    "    accuracy, precision, recall, f1 = metrics\n",
    "    average = average_scores[clf]\n",
    "    print(\"{:<20} {:.2f}      {:.2f}      {:.2f}      {:.2f}      {:.2f}\".format(clf, accuracy * 100, precision * 100, recall * 100, f1 * 100, average * 100))\n",
    "\n",
    "print(\"\\nBest classifier based on average score: {}\".format(best_classifier))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

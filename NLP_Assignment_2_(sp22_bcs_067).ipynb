{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##      Name: Rao Muhammad Rafay\n",
        "##     Reg. No: SP22-BCS-O67\n",
        "##      Section: A"
      ],
      "metadata": {
        "id": "AWpFFTaTZ3ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paragraph"
      ],
      "metadata": {
        "id": "2WcAgodafens"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "s_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "\n",
        "# Paragraphs:\n",
        "text_1=(\"Although the Braille system gained immediate popularity with the blind students at the Institute in Paris,it had to gain acceptance among the sighted before its adoption throughout France.\")\n",
        "\n",
        "text_2=(\"Pakistan, located in South Asia, is a diverse nation rich in culture, history, and landscapes. Its vibrant cities, such as Karachi and Lahore, contrast with its stunning natural beauty, including the majestic Himalayas and Arabian Sea coastline\")\n",
        "\n",
        "\n",
        "doc_1=nlp(text_1)\n",
        "doc_2=nlp(text_2)"
      ],
      "metadata": {
        "id": "AEdbptqZaGhW"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokeinzation"
      ],
      "metadata": {
        "id": "F9b2jiUZcbLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Tokenization(doc):\n",
        "  token = [token.text for token in doc]\n",
        "  return token"
      ],
      "metadata": {
        "id": "m-Y1MYtxiMVk"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### StopWords Removal"
      ],
      "metadata": {
        "id": "_SzenMohdF0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def StopWords_Removal(doc):\n",
        "    stopword_removal = [t.text for t in doc if not  t.is_stop]\n",
        "    return stopword_removal"
      ],
      "metadata": {
        "id": "vf1FQ7SldK_1"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### POS_Tagging"
      ],
      "metadata": {
        "id": "KoC0_qzmeMHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def POS_Tagging(doc):\n",
        "    for t in doc:\n",
        "      print(f'{t.text:{10}} {t.pos_:{10}} {t.tag_:{10}}')"
      ],
      "metadata": {
        "id": "PLz_1rkheMll"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatization"
      ],
      "metadata": {
        "id": "fVVbIVDWerRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Lemmatization(doc):\n",
        "    for token in doc:\n",
        "        print(f'{token.text:{10}} {token.lemma_}')"
      ],
      "metadata": {
        "id": "pRbxVCPYerhj"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stemming"
      ],
      "metadata": {
        "id": "HJqVBszle6Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Stemming(doc):\n",
        "    for word in doc:\n",
        "      print(word+' --> '+s_stemmer.stem(word))\n"
      ],
      "metadata": {
        "id": "cDtP_Aloe6eN"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vocab Function's\n"
      ],
      "metadata": {
        "id": "nx7eorZZfLVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab(doc):\n",
        "    print(\"Length of nlp.vocab:\", len(nlp.vocab))\n",
        "    print(\"nlp.vocab:\", nlp.vocab)\n",
        "    print(\"Length of doc.vocab:\", len(doc.vocab))\n",
        "    print(\"doc.vocab:\", doc.vocab)\n",
        "    for t in range(0,100):\n",
        "      print(nlp.vocab[t].text,end=\" | \")\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "aMTG4XRRfLas"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function"
      ],
      "metadata": {
        "id": "mAzy-oeyjGS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#--------Tokenization of doc_1--------#\n",
        "\n",
        "tokenize_text=Tokenization(doc_1)\n",
        "print(\"--------Tokenization :--------\\n\\n\",tokenize_text, \"\\n\")\n",
        "\n",
        "#--------Stopword Removal of doc_2--------#\n",
        "\n",
        "Without_Stopwords=StopWords_Removal(doc_2)\n",
        "print(\"\\n--------After Removing Stopwords:--------\\n\\n\",Without_Stopwords, \"\\n\")\n",
        "\n",
        "#--------POS Tagging of doc_1--------#\n",
        "\n",
        "print(\"--------POS_Tagging:---------\",end=\"\\n\\n\")\n",
        "POS_Tagging(doc_1)\n",
        "\n",
        "#--------Lemmatization of doc_2--------#\n",
        "\n",
        "print(\"\\n--------Lemmatization:--------\",end=\"\\n\\n\")\n",
        "Lemmatization(doc_2)\n",
        "\n",
        "#--------Stemming--------#\n",
        "\n",
        "print(\"\\n--------Stemming:--------\",end=\"\\n\\n\")\n",
        "words=word_tokenize(text_1)\n",
        "Stemming(words)\n",
        "\n",
        "\n",
        "#--------Vocab Functions--------#\n",
        "\n",
        "print(\"\\n--------Vocab Functions:--------\",end=\"\\n\\n\")\n",
        "vocab(doc_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW-0gjTeqOLa",
        "outputId": "0db4cf8c-e3a6-40b0-94c7-dc8d3aad5bd7"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------Tokenization :--------\n",
            "\n",
            " ['Although', 'the', 'Braille', 'system', 'gained', 'immediate', 'popularity', 'with', 'the', 'blind', 'students', 'at', 'the', 'Institute', 'in', 'Paris', ',', 'it', 'had', 'to', 'gain', 'acceptance', 'among', 'the', 'sighted', 'before', 'its', 'adoption', 'throughout', 'France', '.'] \n",
            "\n",
            "\n",
            "--------After Removing Stopwords:--------\n",
            "\n",
            " ['Pakistan', ',', 'located', 'South', 'Asia', ',', 'diverse', 'nation', 'rich', 'culture', ',', 'history', ',', 'landscapes', '.', 'vibrant', 'cities', ',', 'Karachi', 'Lahore', ',', 'contrast', 'stunning', 'natural', 'beauty', ',', 'including', 'majestic', 'Himalayas', 'Arabian', 'Sea', 'coastline'] \n",
            "\n",
            "--------POS_Tagging:---------\n",
            "\n",
            "Although   SCONJ      IN        \n",
            "the        DET        DT        \n",
            "Braille    PROPN      NNP       \n",
            "system     NOUN       NN        \n",
            "gained     VERB       VBD       \n",
            "immediate  ADJ        JJ        \n",
            "popularity NOUN       NN        \n",
            "with       ADP        IN        \n",
            "the        DET        DT        \n",
            "blind      ADJ        JJ        \n",
            "students   NOUN       NNS       \n",
            "at         ADP        IN        \n",
            "the        DET        DT        \n",
            "Institute  PROPN      NNP       \n",
            "in         ADP        IN        \n",
            "Paris      PROPN      NNP       \n",
            ",          PUNCT      ,         \n",
            "it         PRON       PRP       \n",
            "had        VERB       VBD       \n",
            "to         PART       TO        \n",
            "gain       VERB       VB        \n",
            "acceptance NOUN       NN        \n",
            "among      ADP        IN        \n",
            "the        DET        DT        \n",
            "sighted    ADJ        JJ        \n",
            "before     ADP        IN        \n",
            "its        PRON       PRP$      \n",
            "adoption   NOUN       NN        \n",
            "throughout ADP        IN        \n",
            "France     PROPN      NNP       \n",
            ".          PUNCT      .         \n",
            "\n",
            "--------Lemmatization:--------\n",
            "\n",
            "Pakistan   Pakistan\n",
            ",          ,\n",
            "located    locate\n",
            "in         in\n",
            "South      South\n",
            "Asia       Asia\n",
            ",          ,\n",
            "is         be\n",
            "a          a\n",
            "diverse    diverse\n",
            "nation     nation\n",
            "rich       rich\n",
            "in         in\n",
            "culture    culture\n",
            ",          ,\n",
            "history    history\n",
            ",          ,\n",
            "and        and\n",
            "landscapes landscape\n",
            ".          .\n",
            "Its        its\n",
            "vibrant    vibrant\n",
            "cities     city\n",
            ",          ,\n",
            "such       such\n",
            "as         as\n",
            "Karachi    Karachi\n",
            "and        and\n",
            "Lahore     Lahore\n",
            ",          ,\n",
            "contrast   contrast\n",
            "with       with\n",
            "its        its\n",
            "stunning   stunning\n",
            "natural    natural\n",
            "beauty     beauty\n",
            ",          ,\n",
            "including  include\n",
            "the        the\n",
            "majestic   majestic\n",
            "Himalayas  Himalayas\n",
            "and        and\n",
            "Arabian    Arabian\n",
            "Sea        Sea\n",
            "coastline  coastline\n",
            "\n",
            "--------Stemming:--------\n",
            "\n",
            "Although --> although\n",
            "the --> the\n",
            "Braille --> braill\n",
            "system --> system\n",
            "gained --> gain\n",
            "immediate --> immedi\n",
            "popularity --> popular\n",
            "with --> with\n",
            "the --> the\n",
            "blind --> blind\n",
            "students --> student\n",
            "at --> at\n",
            "the --> the\n",
            "Institute --> institut\n",
            "in --> in\n",
            "Paris --> pari\n",
            ", --> ,\n",
            "it --> it\n",
            "had --> had\n",
            "to --> to\n",
            "gain --> gain\n",
            "acceptance --> accept\n",
            "among --> among\n",
            "the --> the\n",
            "sighted --> sight\n",
            "before --> befor\n",
            "its --> it\n",
            "adoption --> adopt\n",
            "throughout --> throughout\n",
            "France --> franc\n",
            ". --> .\n",
            "\n",
            "--------Vocab Functions:--------\n",
            "\n",
            "Length of nlp.vocab: 915\n",
            "nlp.vocab: <spacy.vocab.Vocab object at 0x7b0003a1b7f0>\n",
            "Length of doc.vocab: 915\n",
            "doc.vocab: <spacy.vocab.Vocab object at 0x7b0003a1b7f0>\n",
            " | IS_ALPHA | IS_ASCII | IS_DIGIT | IS_LOWER | IS_PUNCT | IS_SPACE | IS_TITLE | IS_UPPER | LIKE_URL | LIKE_NUM | LIKE_EMAIL | IS_STOP | IS_OOV_DEPRECATED | IS_BRACKET | IS_QUOTE | IS_LEFT_PUNCT | IS_RIGHT_PUNCT | IS_CURRENCY | FLAG19 | FLAG20 | FLAG21 | FLAG22 | FLAG23 | FLAG24 | FLAG25 | FLAG26 | FLAG27 | FLAG28 | FLAG29 | FLAG30 | FLAG31 | FLAG32 | FLAG33 | FLAG34 | FLAG35 | FLAG36 | FLAG37 | FLAG38 | FLAG39 | FLAG40 | FLAG41 | FLAG42 | FLAG43 | FLAG44 | FLAG45 | FLAG46 | FLAG47 | FLAG48 | FLAG49 | FLAG50 | FLAG51 | FLAG52 | FLAG53 | FLAG54 | FLAG55 | FLAG56 | FLAG57 | FLAG58 | FLAG59 | FLAG60 | FLAG61 | FLAG62 | FLAG63 | ID | ORTH | LOWER | NORM | SHAPE | PREFIX | SUFFIX | LENGTH | CLUSTER | LEMMA | POS | TAG | DEP | ENT_IOB | ENT_TYPE | HEAD | SENT_START | SPACY | PROB | LANG | ADJ | ADP | ADV | AUX | CONJ | CCONJ | DET | INTJ | NOUN | NUM | PART | PRON | PROPN | PUNCT | SCONJ | SYM | "
          ]
        }
      ]
    }
  ]
}